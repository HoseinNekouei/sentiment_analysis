{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/sentiment_analysis/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # For better error messages"
      ],
      "metadata": {
        "id": "hVKumCSPhwhb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AfELiPnbJiwU",
        "outputId": "4107257f-8512-453d-ba97-bf322deb555d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysisModel:\n",
        "    def __init__(self, checkpoint, epochs=2, batch_size=32, n_splits=5):\n",
        "        self.checkpoint = checkpoint\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.n_splits = n_splits\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.tokenizer, self.model = self.load_model_and_tokenizer()\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=5e-5)\n",
        "\n",
        "    def load_model_and_tokenizer(self):\n",
        "        \"\"\"Load the tokenizer and model from the specified checkpoint.\"\"\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(self.checkpoint)\n",
        "\n",
        "        model.to(self.device)\n",
        "        return tokenizer, model\n",
        "\n",
        "    def tokenize_texts(self, texts):\n",
        "        \"\"\"Tokenize a list of texts using the provided tokenizer.\"\"\"\n",
        "        if not isinstance(texts, list):\n",
        "            texts = texts.tolist()\n",
        "\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            max_length=256,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return encodings\n",
        "\n",
        "    def prepare_dataloaders(self, input_ids, attention_mask, labels, train_idx, test_idx):\n",
        "        \"\"\"Prepare training and test dataloaders.\"\"\"\n",
        "        train_dataset = TensorDataset(input_ids[train_idx], attention_mask[train_idx], labels[train_idx])\n",
        "        test_dataset = TensorDataset(input_ids[test_idx], attention_mask[test_idx], labels[test_idx])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def train_model(self, train_loader):\n",
        "        \"\"\"Train the model for the specified number of epochs.\"\"\"\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                self.optimizer.zero_grad()\n",
        "                input_ids_batch, attention_mask_batch, labels_batch = [b.to(self.device) for b in batch]\n",
        "                outputs = self.model(input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                denominator = math.ceil(len(train_loader.dataset) / self.batch_size)\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    print(f'[Epoch: {epoch + 1}] -> Batch: [{i + 1}/{denominator}]')\n",
        "\n",
        "            print(f'[Epoch: {epoch + 1}] -> Batch: [{denominator}/{denominator}]')\n",
        "\n",
        "    def evaluate_model(self, test_loader):\n",
        "        \"\"\"Evaluate the model on the validation set and return accuracy.\"\"\"\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds= []\n",
        "        all_labels= []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids_batch, attention_mask_batch, labels_batch = [b.to(self.device) for b in batch]\n",
        "\n",
        "                outputs = self.model(input_ids_batch, attention_mask=attention_mask_batch)\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                probs= torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "                all_preds.append(probs)\n",
        "                all_labels.append(labels_batch)\n",
        "\n",
        "                correct += torch.sum(preds == labels_batch).item()\n",
        "                total += len(labels_batch)\n",
        "\n",
        "        # Concatenate all predictions and labels\n",
        "        all_preds = torch.cat(all_preds, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        # Calculate F1-score\n",
        "        f1 = self.calculate_f1_score(all_preds, all_labels)\n",
        "\n",
        "        return accuracy, f1, all_preds, all_labels\n",
        "\n",
        "\n",
        "    def calculate_f1_score(self, preds, labels):\n",
        "        return f1_score(labels.cpu().numpy(),\n",
        "                   torch.argmax(preds, dim=1).cpu().numpy(),\n",
        "                   average='macro')\n",
        "\n",
        "\n",
        "    def plot_mean_roc_curve(self, all_folds_preds, all_folds_labels, n_classes):\n",
        "        \"\"\"\n",
        "        Plot the mean ROC curve across all folds of cross-validation.\n",
        "\n",
        "        Args:\n",
        "            all_folds_preds: List of prediction tensors from each fold\n",
        "            all_folds_labels: List of label tensors from each fold\n",
        "            n_classes: Number of classes\n",
        "        \"\"\"\n",
        "        # Initialize dictionaries to store ROC metrics for each class\n",
        "        mean_fpr = np.linspace(0, 1, 100)\n",
        "        tprs = [[] for _ in range(n_classes)]\n",
        "        aucs = [[] for _ in range(n_classes)]\n",
        "\n",
        "        # Calculate ROC for each fold\n",
        "        for fold_preds, fold_labels in zip(all_folds_preds, all_folds_labels):\n",
        "            for i in range(n_classes):\n",
        "                fpr, tpr, _ = roc_curve(fold_labels.cpu().numpy() == i,\n",
        "                                      fold_preds[:, i].cpu().numpy())\n",
        "                interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "                interp_tpr[0] = 0.0\n",
        "                tprs[i].append(interp_tpr)\n",
        "                aucs[i].append(auc(fpr, tpr))\n",
        "\n",
        "        # Calculate mean and standard deviation\n",
        "        mean_tpr = [np.mean(tprs[i], axis=0) for i in range(n_classes)]\n",
        "        mean_auc = [np.mean(aucs[i]) for i in range(n_classes)]\n",
        "        std_auc = [np.std(aucs[i]) for i in range(n_classes)]\n",
        "\n",
        "        # Plot ROC curves\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        colors = ['blue', 'green', 'red', 'cyan', 'magenta']\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            # Plot mean ROC\n",
        "            plt.plot(mean_fpr, mean_tpr[i], color=colors[i%len(colors)],\n",
        "                    label=f'Class {i} (Mean AUC = {mean_auc[i]:.2f} Â± {std_auc[i]:.2f})')\n",
        "\n",
        "            # Plot standard deviation (optional)\n",
        "            std_tpr = np.std(tprs[i], axis=0)\n",
        "            tprs_upper = np.minimum(mean_tpr[i] + std_tpr, 1)\n",
        "            tprs_lower = np.maximum(mean_tpr[i] - std_tpr, 0)\n",
        "            plt.fill_between(mean_fpr, tprs_lower, tprs_upper,\n",
        "                            color=colors[i%len(colors)], alpha=0.2)\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Mean ROC Curve Across All Folds (Â±1 SD)')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def kfold_cross_validation(self, train_data):\n",
        "        \"\"\"Perform k-fold cross-validation on the provided dataset.\"\"\"\n",
        "\n",
        "        # Tokenize texts and prepare labels\n",
        "        texts = train_data['text']\n",
        "        labels = torch.tensor(train_data['labels'].tolist()).to(self.device)\n",
        "        n_classes = len(torch.unique(labels))\n",
        "\n",
        "        # Verify labels\n",
        "        print(\"Unique labels:\", torch.unique(labels))\n",
        "        print(\"Label value range:\", labels.min(), labels.max())\n",
        "        assert labels.min() >= 0 and labels.max() < n_classes, \"Invalid label values\"\n",
        "\n",
        "        encodings = self.tokenize_texts(texts)\n",
        "        input_ids = encodings['input_ids'].to(self.device)\n",
        "        attention_mask = encodings['attention_mask'].to(self.device)\n",
        "\n",
        "        # Initialize KFold\n",
        "        kfold = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
        "        accuracy_scores = []\n",
        "        f1_scores = []\n",
        "        all_preds_by_fold = []\n",
        "        all_labels_by_fold = []\n",
        "\n",
        "        # Perform k-fold cross-validation\n",
        "        for fold, (train_idx, test_idx) in enumerate(kfold.split(input_ids)):\n",
        "            print(f\"\\nFold {fold + 1}/{self.n_splits}\")\n",
        "\n",
        "            # Prepare dataloaders\n",
        "            train_loader, test_loader = self.prepare_dataloaders(input_ids, attention_mask, labels, train_idx, test_idx)\n",
        "\n",
        "            # Train the model\n",
        "            self.train_model(train_loader)\n",
        "\n",
        "            # Evaluate the model\n",
        "            fold_metrics = self.evaluate_model(test_loader)\n",
        "            accuracy_scores.append(fold_metrics['accuracy'])\n",
        "            f1_scores.append(fold_metrics['f1'])\n",
        "            all_preds_by_fold.append(fold_metrics['preds'])\n",
        "            all_labels_by_fold.append(fold_metrics['labels'])\n",
        "\n",
        "            print(f\"Validation Accuracy: {fold_metrics['accuracy']:.4f}\")\n",
        "            print(f\"Validation F1-Score: {fold_metrics['f1']:.4f}\")\n",
        "            print('=' * 30)\n",
        "\n",
        "        # Calculate and print final results\n",
        "        accuracy_scores = torch.tensor(accuracy_scores)\n",
        "        f1_scores = torch.tensor(f1_scores)\n",
        "        print(f\"\\nFinal Results:\")\n",
        "        print(f\"Mean accuracy: {torch.mean(accuracy_scores):.4f} Â± {torch.std(accuracy_scores):.4f}\")\n",
        "        print(f\"Mean F1-score: {torch.mean(f1_scores):.4f} Â± {torch.std(f1_scores):.4f}\")\n",
        "\n",
        "        # Plot ROC curve\n",
        "        self.plot_mean_roc_curve(all_preds_by_fold, all_labels_by_fold, n_classes)\n",
        "\n",
        "        model_save_path = '/content/drive/MyDrive/Projects/Sentiment_Analysis/IMDB_sentiment_model_batch32_K5.pt'\n",
        "        torch.save(self.model.state_dict(), model_save_path)  # Save state_dict instead of full model\n",
        "        print(f\"\\nModel saved to {model_save_path}!\")"
      ],
      "metadata": {
        "id": "3pAxqrJbkVPU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Clone the repository and load the data\n",
        "    !git clone 'https://github.com/HoseinNekouei/sentiment_analysis.git'\n",
        "    file_path = '/content/sentiment_analysis/twitter_train_corpus.csv'\n",
        "    # file_path = '/content/drive/MyDrive/dataset/aspected_stanfordnlp_IMDB_train_corpus.csv'\n",
        "\n",
        "    # Constants\n",
        "    nrows= 1000\n",
        "    N_SPLITS= 5 # k-fold\n",
        "    EPOCHS = 2\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_data = pd.read_csv(file_path, nrows= nrows) # Load your training data\n",
        "    print(train_data.info())\n",
        "\n",
        "    # Initialize the SentimentAnalysisModel class\n",
        "    # checkpoint= 'FacebookAI/roberta-base'\n",
        "    checkpoint= 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
        "    model = SentimentAnalysisModel(checkpoint, batch_size= BATCH_SIZE, epochs= EPOCHS, n_splits= N_SPLITS)\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    model.kfold_cross_validation(train_data)"
      ],
      "metadata": {
        "id": "5JpB_cJZkq86",
        "outputId": "76e58c64-ad97-4c72-a564-d466618baaa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sentiment_analysis' already exists and is not an empty directory.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    1000 non-null   object\n",
            " 1   labels  1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: tensor([0, 1, 2], device='cuda:0')\n",
            "Label value range: tensor(0, device='cuda:0') tensor(2, device='cuda:0')\n",
            "\n",
            "Fold 1/5\n",
            "[Epoch: 1] -> Batch: [10/25]\n",
            "[Epoch: 1] -> Batch: [20/25]\n",
            "[Epoch: 1] -> Batch: [25/25]\n",
            "[Epoch: 2] -> Batch: [10/25]\n",
            "[Epoch: 2] -> Batch: [20/25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMGiS4i_fuRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}