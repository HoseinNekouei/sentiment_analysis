{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO47O6jTIBzdCpNfese2nNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoseinNekouei/sentiment_analysis/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AfELiPnbJiwU",
        "outputId": "93056cb7-80b5-457e-c40a-7bbfa0ae2a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysisModel:\n",
        "    def __init__(self, checkpoint, epochs=2, batch_size=32, n_splits=5):\n",
        "        self.checkpoint = checkpoint\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.n_splits = n_splits\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.tokenizer, self.model = self.load_model_and_tokenizer()\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=5e-5)\n",
        "\n",
        "    def load_model_and_tokenizer(self):\n",
        "        \"\"\"Load the tokenizer and model from the specified checkpoint.\"\"\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(self.checkpoint, device_map='auto')\n",
        "        model.to(self.device)\n",
        "        return tokenizer, model\n",
        "\n",
        "    def tokenize_texts(self, texts):\n",
        "        \"\"\"Tokenize a list of texts using the provided tokenizer.\"\"\"\n",
        "        if not isinstance(texts, list):\n",
        "            texts = texts.tolist()\n",
        "\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            max_length=256,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return encodings\n",
        "\n",
        "    def prepare_dataloaders(self, input_ids, attention_mask, labels, train_idx, test_idx):\n",
        "        \"\"\"Prepare training and test dataloaders.\"\"\"\n",
        "        train_dataset = TensorDataset(input_ids[train_idx], attention_mask[train_idx], labels[train_idx])\n",
        "        test_dataset = TensorDataset(input_ids[test_idx], attention_mask[test_idx], labels[test_idx])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def train_model(self, train_loader):\n",
        "        \"\"\"Train the model for the specified number of epochs.\"\"\"\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                self.optimizer.zero_grad()\n",
        "                input_ids_batch, attention_mask_batch, labels_batch = [b.to(self.device) for b in batch]\n",
        "                outputs = self.model(input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    denominator = math.ceil(len(train_loader.dataset) / self.batch_size)\n",
        "                    print(f'[Epoch: {epoch + 1}] -> Batch: [{i + 1}/{denominator}]')\n",
        "\n",
        "            print(f'[Epoch: {epoch + 1}] -> Batch: [{denominator}/{denominator}]')\n",
        "\n",
        "    def evaluate_model(self, test_loader):\n",
        "        \"\"\"Evaluate the model on the validation set and return accuracy.\"\"\"\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds= []\n",
        "        all_labels= []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids_batch, attention_mask_batch, labels_batch = [b.to(self.device) for b in batch]\n",
        "\n",
        "                outputs = self.model(input_ids_batch, attention_mask=attention_mask_batch)\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                probs= torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "                all_preds.append(probs)\n",
        "                all_labels.append(labels_batch)\n",
        "\n",
        "                correct += torch.sum(preds == labels_batch).item()\n",
        "                total += len(labels_batch)\n",
        "\n",
        "        # Concatenate all predictions and labels\n",
        "        all_preds = torch.cat(all_preds, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "\n",
        "        # Calculate F1-score\n",
        "        f1 = self.calculate_f1_score(all_preds, all_labels)\n",
        "\n",
        "        return accuracy, f1, all_preds, all_labels\n",
        "\n",
        "    def calculate_f1_score(self, preds, labels):\n",
        "        \"\"\"Calculate the F1-score for multi-class classification.\"\"\"\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        tp = (preds == labels).sum().item()\n",
        "        fp = (preds != labels).sum().item()\n",
        "        fn = (labels != preds).sum().item()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        return f1\n",
        "\n",
        "    def plot_roc_curve(self, y_true, y_scores, n_classes):\n",
        "        \"\"\"Plot the ROC curve for multi-class classification.\"\"\"\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_true.cpu().numpy() == i, y_scores[:, i].cpu().numpy())\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Plot ROC curve\n",
        "        plt.figure()\n",
        "        for i in range(n_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "    def kfold_cross_validation(self, train_data):\n",
        "        \"\"\"Perform k-fold cross-validation on the provided dataset.\"\"\"\n",
        "        # Tokenize texts and prepare labels\n",
        "        texts = train_data['text']\n",
        "        labels = torch.tensor(train_data['labels'].tolist()).to(self.device)\n",
        "        encodings = self.tokenize_texts(texts)\n",
        "        input_ids = encodings['input_ids'].to(self.device)\n",
        "        attention_mask = encodings['attention_mask'].to(self.device)\n",
        "\n",
        "        # Initialize KFold\n",
        "        kfold = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
        "        accuracy_scores = []\n",
        "        f1_scores = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        # Perform k-fold cross-validation\n",
        "        for fold, (train_idx, test_idx) in enumerate(kfold.split(input_ids)):\n",
        "            print(f\"Fold {fold + 1}...\")\n",
        "\n",
        "            # Prepare dataloaders\n",
        "            train_loader, test_loader = self.prepare_dataloaders(input_ids, attention_mask, labels, train_idx, test_idx)\n",
        "\n",
        "            # Train the model\n",
        "            self.train_model(train_loader)\n",
        "\n",
        "            # Evaluate the model\n",
        "            fold_accuracy, fold_f1, fold_preds, fold_labels = self.evaluate_model(test_loader)\n",
        "            accuracy_scores.append(fold_accuracy)\n",
        "            f1_scores.append(fold_f1)\n",
        "            all_preds.append(fold_preds)\n",
        "            all_labels.append(fold_labels)\n",
        "\n",
        "            print(f\"Validation Accuracy: {fold_accuracy:.4f}\")\n",
        "            print(f\"Validation F1-Score: {fold_f1:.4f}\")\n",
        "            print('=' * 30)\n",
        "\n",
        "        # Calculate and print final results\n",
        "        accuracy_scores = torch.tensor(accuracy_scores)\n",
        "        f1_scores = torch.tensor(f1_scores)\n",
        "        print(f\"Cross-validation accuracy scores: {accuracy_scores}\")\n",
        "        print(f\"Mean accuracy: {torch.mean(accuracy_scores):.4f}\")\n",
        "        print(f\"Standard deviation (accuracy): {torch.std(accuracy_scores):.4f}\")\n",
        "        print(f\"Cross-validation F1-scores: {f1_scores}\")\n",
        "        print(f\"Mean F1-score: {torch.mean(f1_scores):.4f}\")\n",
        "        print(f\"Standard deviation (F1-score): {torch.std(f1_scores):.4f}\")\n",
        "\n",
        "        # Concatenate predictions and labels from all folds\n",
        "        all_preds = torch.cat(all_preds, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Plot ROC curve\n",
        "        n_classes = len(torch.unique(all_labels))\n",
        "        self.plot_roc_curve(all_labels, all_preds, n_classes)\n",
        "\n",
        "        # Save the model\n",
        "        model_save_path = '/content/sentiment_analysis/sentiment_model_batch32_K5.pt'\n",
        "        torch.save(self.model, model_save_path)\n",
        "        print(f\"Model saved to {model_save_path}!\")"
      ],
      "metadata": {
        "id": "3pAxqrJbkVPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Clone the repository and load the data\n",
        "    !git clone 'https://github.com/HoseinNekouei/sentiment_analysis.git'\n",
        "    file_path = '/content/sentiment_analysis/twitter_train_corpus.csv'\n",
        "\n",
        "    # Constants\n",
        "    nrows= None\n",
        "    N_SPLITS= 5 # k-fold\n",
        "    EPOCHS = 2\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    train_data = pd.read_csv(file_path, nrows= nrows) # Load your training data\n",
        "    print(train_data.info())\n",
        "\n",
        "    # Initialize the SentimentAnalysisModel class\n",
        "    checkpoint= 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
        "    model = SentimentAnalysisModel(checkpoint, batch_size= BATCH_SIZE, epochs= EPOCHS, n_splits= N_SPLITS)\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    model.kfold_cross_validation(train_data)"
      ],
      "metadata": {
        "id": "5JpB_cJZkq86",
        "outputId": "e835271c-b5f0-4731-f88e-c52b04f6233a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sentiment_analysis' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1...\n",
            "[Epoch: 1] -> Batch: [1/5]\n",
            "[Epoch: 1] -> Batch: [2/5]\n",
            "[Epoch: 1] -> Batch: [3/5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImDqQiyUKRcB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}